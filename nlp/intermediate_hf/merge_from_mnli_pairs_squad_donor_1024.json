[
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 1,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 87.05392080744258,
        "spearman_corrcoef": 86.96717989095119
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 87.22294933495407,
        "spearman_corrcoef": 87.12149267102929
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": 0,
    "original_score": {
      "mrpc": {
        "f1": 88.85077186963979,
        "accuracy": 84.06862745098039
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 88.85077186963979,
        "accuracy": 84.06862745098039
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 2,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 87.33002856748678,
        "spearman_corrcoef": 87.08897567009578
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 87.33002856748678,
        "spearman_corrcoef": 87.08897567009578
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 4,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 87.61903390854961,
        "spearman_corrcoef": 87.51633852635744
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 87.71639377120192,
        "spearman_corrcoef": 87.65488040538318
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 0,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 86.21696839791932,
        "spearman_corrcoef": 86.00448308980644
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 86.21696839791932,
        "spearman_corrcoef": 86.00448308980644
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 3,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 87.1222342028868,
        "spearman_corrcoef": 87.08422349781387
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 87.32672495360023,
        "spearman_corrcoef": 87.22413371975843
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": 3,
    "original_score": {
      "mrpc": {
        "f1": 88.77551020408163,
        "accuracy": 83.82352941176471
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 89.1891891891892,
        "accuracy": 84.31372549019608
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": 2,
    "original_score": {
      "mrpc": {
        "f1": 86.4963503649635,
        "accuracy": 81.86274509803921
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 88.2249560632689,
        "accuracy": 83.57843137254902
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.9
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": 1,
    "original_score": {
      "mrpc": {
        "f1": 87.45762711864407,
        "accuracy": 81.86274509803921
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 87.87878787878788,
        "accuracy": 82.35294117647058
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 1,
    "original_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.58
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 0,
    "original_score": {
      "rte": {
        "accuracy": 78.70036101083032
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": 4,
    "original_score": {
      "mrpc": {
        "f1": 88.77374784110535,
        "accuracy": 84.06862745098039
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 89.3835616438356,
        "accuracy": 84.80392156862744
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 3,
    "original_score": {
      "rte": {
        "accuracy": 77.25631768953069
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.68
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 4,
    "original_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 2,
    "original_score": {
      "rte": {
        "accuracy": 78.33935018050542
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.48
  }
]