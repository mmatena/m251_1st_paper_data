[
  {
    "target_task": "cola",
    "donor_task": "rte",
    "trial_index": 2,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 58.10381850664859
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 58.10381850664859
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "cola",
    "donor_task": "rte",
    "trial_index": 1,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 56.831496658025735
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 56.831496658025735
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "cola",
    "donor_task": "rte",
    "trial_index": 3,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 54.4421655829423
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 54.94968980251971
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "cola",
    "donor_task": "rte",
    "trial_index": 4,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 54.60878038039295
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 54.60878038039295
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": 1,
    "original_score": {
      "mrpc": {
        "f1": 87.00361010830325,
        "accuracy": 82.35294117647058
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 87.90459965928449,
        "accuracy": 82.59803921568627
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.88
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 1,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 87.3320166018849,
        "spearman_corrcoef": 86.6466895818426
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 87.44791786214041,
        "spearman_corrcoef": 86.68027183020727
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 0,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 86.2620775427326,
        "spearman_corrcoef": 85.78916237896371
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 86.2620775427326,
        "spearman_corrcoef": 85.78916237896371
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": 2,
    "original_score": {
      "mrpc": {
        "f1": 87.45762711864407,
        "accuracy": 81.86274509803921
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 87.54208754208753,
        "accuracy": 81.86274509803921
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 1,
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 2,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 85.55687160801008,
        "spearman_corrcoef": 85.2284537815866
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 85.55687160801008,
        "spearman_corrcoef": 85.2284537815866
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 3,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 87.29248143977159,
        "spearman_corrcoef": 86.7652457866537
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 87.29248143977159,
        "spearman_corrcoef": 86.7652457866537
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": 3,
    "original_score": {
      "mrpc": {
        "f1": 87.58389261744966,
        "accuracy": 81.86274509803921
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 88.38709677419355,
        "accuracy": 82.35294117647058
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.82
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 3,
    "original_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.82
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 2,
    "original_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.26
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 0,
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.36
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": 4,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 85.10036990495374,
        "spearman_corrcoef": 84.66275567672378
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 85.10036990495374,
        "spearman_corrcoef": 84.66275567672378
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": 4,
    "original_score": {
      "mrpc": {
        "f1": 87.12871287128714,
        "accuracy": 80.88235294117648
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 87.2053872053872,
        "accuracy": 81.37254901960785
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "rte",
    "donor_task": "rte",
    "trial_index": 4,
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.62093862815884
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  }
]