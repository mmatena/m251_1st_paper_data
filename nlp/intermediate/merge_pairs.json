[
  {
    "target_task": "rte",
    "donor_task": "cola",
    "trial_index": 2,
    "original_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "target_task": "rte",
    "donor_task": "sst2",
    "trial_index": 2,
    "original_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 79.42238267148014
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.78
  },
  {
    "target_task": "rte",
    "donor_task": "stsb",
    "trial_index": 2,
    "original_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "stsb",
    "trial_index": 0,
    "original_score": {
      "rte": {
        "accuracy": 80.50541516245488
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 80.86642599277978
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "target_task": "rte",
    "donor_task": "sst2",
    "trial_index": 0,
    "original_score": {
      "rte": {
        "accuracy": 80.50541516245488
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 80.86642599277978
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.86
  },
  {
    "target_task": "rte",
    "donor_task": "mrpc",
    "trial_index": 0,
    "original_score": {
      "rte": {
        "accuracy": 80.50541516245488
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 82.31046931407943
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.54
  },
  {
    "target_task": "cola",
    "donor_task": "stsb",
    "trial_index": 2,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 63.22856025423966
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "cola",
    "donor_task": "sst2",
    "trial_index": 2,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "cola",
    "donor_task": "rte",
    "trial_index": 1,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.32614350622284
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.645571917265286
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.9
  },
  {
    "target_task": "cola",
    "donor_task": "stsb",
    "trial_index": 1,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.32614350622284
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.86058869114631
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "cola",
    "donor_task": "mrpc",
    "trial_index": 1,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.32614350622284
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 63.36846352657527
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.78
  },
  {
    "target_task": "rte",
    "donor_task": "stsb",
    "trial_index": 3,
    "original_score": {
      "rte": {
        "accuracy": 83.39350180505414
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 85.92057761732852
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.06
  },
  {
    "target_task": "cola",
    "donor_task": "sst2",
    "trial_index": 1,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.32614350622284
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.32614350622284
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "mrpc",
    "trial_index": 3,
    "original_score": {
      "rte": {
        "accuracy": 83.39350180505414
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 84.47653429602889
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.76
  },
  {
    "target_task": "rte",
    "donor_task": "sst2",
    "trial_index": 3,
    "original_score": {
      "rte": {
        "accuracy": 83.39350180505414
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 85.5595667870036
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "target_task": "rte",
    "donor_task": "cola",
    "trial_index": 3,
    "original_score": {
      "rte": {
        "accuracy": 83.39350180505414
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 85.92057761732852
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.52
  },
  {
    "target_task": "rte",
    "donor_task": "mrpc",
    "trial_index": 1,
    "original_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 84.83754512635379
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.86
  },
  {
    "target_task": "rte",
    "donor_task": "stsb",
    "trial_index": 4,
    "original_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 83.39350180505414
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "rte",
    "donor_task": "mrpc",
    "trial_index": 4,
    "original_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 83.03249097472924
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "target_task": "rte",
    "donor_task": "sst2",
    "trial_index": 4,
    "original_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "rte",
    "donor_task": "stsb",
    "trial_index": 1,
    "original_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "rte",
    "donor_task": "sst2",
    "trial_index": 1,
    "original_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 83.03249097472924
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "mrpc",
    "donor_task": "stsb",
    "trial_index": 1,
    "original_score": {
      "mrpc": {
        "f1": 92.7689594356261,
        "accuracy": 89.95098039215686
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.33333333333333,
        "accuracy": 90.68627450980392
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "mrpc",
    "donor_task": "sst2",
    "trial_index": 1,
    "original_score": {
      "mrpc": {
        "f1": 92.7689594356261,
        "accuracy": 89.95098039215686
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 92.7689594356261,
        "accuracy": 89.95098039215686
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "rte",
    "trial_index": "rev_0",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.69266055045871
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "cola",
    "donor_task": "rte",
    "trial_index": "rev_2",
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 63.01194851406684
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": "rev_0",
    "original_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": "rev_2",
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 89.61007975390662,
        "spearman_corrcoef": 89.45270247817778
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.58605141490953,
        "spearman_corrcoef": 89.48652805200422
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": "rev_0",
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.96830725441653,
        "spearman_corrcoef": 88.67616503909271
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.96830725441653,
        "spearman_corrcoef": 88.67616503909271
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "rte",
    "trial_index": "rev_2",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.80733944954129
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "stsb",
    "donor_task": "cola",
    "trial_index": "rev_2",
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 89.61007975390662,
        "spearman_corrcoef": 89.45270247817778
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.61007975390662,
        "spearman_corrcoef": 89.45270247817778
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "cola",
    "trial_index": "rev_1",
    "original_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 83.39350180505414
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "mrpc",
    "donor_task": "cola",
    "trial_index": "rev_1",
    "original_score": {
      "mrpc": {
        "f1": 92.7689594356261,
        "accuracy": 89.95098039215686
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 92.98245614035086,
        "accuracy": 90.19607843137256
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "sst2",
    "donor_task": "cola",
    "trial_index": "rev_1",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.69266055045871
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.86
  },
  {
    "target_task": "stsb",
    "donor_task": "cola",
    "trial_index": "rev_1",
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": "rev_3",
    "original_score": {
      "mrpc": {
        "f1": 89.83957219251337,
        "accuracy": 86.02941176470588
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 90.81272084805654,
        "accuracy": 87.25490196078431
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": "rev_3",
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "cola",
    "donor_task": "rte",
    "trial_index": "rev_3",
    "original_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 59.62108323383991
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "sst2",
    "donor_task": "rte",
    "trial_index": "rev_3",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.80733944954129
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": "rev_4",
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 90.34995245052271,
        "spearman_corrcoef": 90.2020417053629
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 90.55282410943282,
        "spearman_corrcoef": 90.4247795682006
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": "rev_4",
    "original_score": {
      "mrpc": {
        "f1": 92.2242314647378,
        "accuracy": 89.4607843137255
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 92.5,
        "accuracy": 89.70588235294117
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.78
  },
  {
    "target_task": "sst2",
    "donor_task": "rte",
    "trial_index": "rev_4",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.69266055045871
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.6
  },
  {
    "target_task": "mrpc",
    "donor_task": "rte",
    "trial_index": "rev_1",
    "original_score": {
      "mrpc": {
        "f1": 92.7689594356261,
        "accuracy": 89.95098039215686
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.006993006993,
        "accuracy": 90.19607843137256
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "sst2",
    "donor_task": "rte",
    "trial_index": "rev_1",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.80733944954129
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.9
  },
  {
    "target_task": "stsb",
    "donor_task": "sst2",
    "trial_index": 0,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.96830725441653,
        "spearman_corrcoef": 88.67616503909271
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.21096476090946,
        "spearman_corrcoef": 88.83849615206285
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "sst2",
    "donor_task": "stsb",
    "trial_index": "rev_0",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.80733944954129
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "stsb",
    "donor_task": "rte",
    "trial_index": "rev_1",
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "mrpc",
    "trial_index": 0,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.96830725441653,
        "spearman_corrcoef": 88.67616503909271
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.12773150340122,
        "spearman_corrcoef": 88.80763457539585
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "mrpc",
    "donor_task": "stsb",
    "trial_index": "rev_0",
    "original_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "stsb",
    "trial_index": "rev_4",
    "original_score": {
      "mrpc": {
        "f1": 92.2242314647378,
        "accuracy": 89.4607843137255
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 92.63913824057451,
        "accuracy": 89.95098039215686
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "stsb",
    "donor_task": "mrpc",
    "trial_index": 4,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 90.34995245052271,
        "spearman_corrcoef": 90.2020417053629
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 90.49157332416145,
        "spearman_corrcoef": 90.3470386391282
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "stsb",
    "donor_task": "sst2",
    "trial_index": 4,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 90.34995245052271,
        "spearman_corrcoef": 90.2020417053629
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 90.59680118634934,
        "spearman_corrcoef": 90.44455756330505
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "stsb",
    "donor_task": "mrpc",
    "trial_index": "rev_1",
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "stsb",
    "trial_index": "rev_4",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.92201834862385
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "sst2",
    "donor_task": "mrpc",
    "trial_index": "rev_1",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "stsb",
    "trial_index": "rev_3",
    "original_score": {
      "mrpc": {
        "f1": 89.83957219251337,
        "accuracy": 86.02941176470588
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 90.81272084805654,
        "accuracy": 87.25490196078431
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.44
  },
  {
    "target_task": "stsb",
    "donor_task": "sst2",
    "trial_index": 3,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "mrpc",
    "trial_index": 3,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "cola",
    "trial_index": 3,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "stsb",
    "trial_index": "rev_3",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.80733944954129
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.34
  },
  {
    "target_task": "cola",
    "donor_task": "stsb",
    "trial_index": "rev_3",
    "original_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 60.06530974238766
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "mrpc",
    "donor_task": "sst2",
    "trial_index": 3,
    "original_score": {
      "mrpc": {
        "f1": 89.83957219251337,
        "accuracy": 86.02941176470588
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 90.42553191489361,
        "accuracy": 86.76470588235294
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.8
  },
  {
    "target_task": "sst2",
    "donor_task": "mrpc",
    "trial_index": "rev_3",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "cola",
    "trial_index": 3,
    "original_score": {
      "mrpc": {
        "f1": 89.83957219251337,
        "accuracy": 86.02941176470588
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 90.59233449477352,
        "accuracy": 86.76470588235294
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.86
  },
  {
    "target_task": "mrpc",
    "donor_task": "sst2",
    "trial_index": 4,
    "original_score": {
      "mrpc": {
        "f1": 92.2242314647378,
        "accuracy": 89.4607843137255
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 92.3076923076923,
        "accuracy": 89.4607843137255
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "sst2",
    "donor_task": "mrpc",
    "trial_index": "rev_4",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.69266055045871
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "cola",
    "donor_task": "mrpc",
    "trial_index": "rev_3",
    "original_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "stsb",
    "trial_index": "rev_2",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.92201834862385
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "stsb",
    "donor_task": "sst2",
    "trial_index": 2,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 89.61007975390662,
        "spearman_corrcoef": 89.45270247817778
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.61007975390662,
        "spearman_corrcoef": 89.45270247817778
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "sst2",
    "trial_index": 1,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "cola",
    "trial_index": 3,
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.69266055045871
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "sst2",
    "donor_task": "stsb",
    "trial_index": "rev_1",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.80733944954129
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "cola",
    "donor_task": "sst2",
    "trial_index": "rev_3",
    "original_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "mrpc",
    "trial_index": 0,
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "sst2",
    "trial_index": "rev_0",
    "original_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "cola",
    "trial_index": 0,
    "original_score": {
      "rte": {
        "accuracy": 80.50541516245488
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 81.58844765342961
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "target_task": "cola",
    "donor_task": "rte",
    "trial_index": "rev_0",
    "original_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "cola",
    "trial_index": "rev_2",
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.46330275229357
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "stsb",
    "donor_task": "cola",
    "trial_index": 0,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.96830725441653,
        "spearman_corrcoef": 88.67616503909271
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.96830725441653,
        "spearman_corrcoef": 88.67616503909271
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "cola",
    "donor_task": "stsb",
    "trial_index": "rev_0",
    "original_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "cola",
    "trial_index": 0,
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.46330275229357
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.78
  },
  {
    "target_task": "cola",
    "donor_task": "sst2",
    "trial_index": "rev_0",
    "original_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "cola",
    "trial_index": 0,
    "original_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "cola",
    "donor_task": "mrpc",
    "trial_index": "rev_0",
    "original_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "qqp",
    "trial_index": 0,
    "original_score": {
      "rte": {
        "accuracy": 80.50541516245488
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 81.58844765342961
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.82
  },
  {
    "target_task": "cola",
    "donor_task": "qqp",
    "trial_index": 1,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.32614350622284
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.60050070936284
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "qqp",
    "donor_task": "cola",
    "trial_index": "rev_1",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.0499052432091,
        "accuracy": 89.990234375
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "cola",
    "donor_task": "qqp",
    "trial_index": 2,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "qqp",
    "trial_index": 3,
    "original_score": {
      "rte": {
        "accuracy": 83.39350180505414
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 84.47653429602889
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "target_task": "qqp",
    "donor_task": "rte",
    "trial_index": "rev_2",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.1859296482412,
        "accuracy": 90.0390625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "rte",
    "donor_task": "qqp",
    "trial_index": 2,
    "original_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "target_task": "qqp",
    "donor_task": "rte",
    "trial_index": "rev_0",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.07653701380175,
        "accuracy": 89.94140625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "qqp",
    "donor_task": "cola",
    "trial_index": "rev_2",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.12121212121212,
        "accuracy": 90.0390625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "rte",
    "donor_task": "qqp",
    "trial_index": 1,
    "original_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 83.03249097472924
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "qqp",
    "donor_task": "rte",
    "trial_index": "rev_3",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.32747804265998,
        "accuracy": 90.13671875
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "rte",
    "donor_task": "qqp",
    "trial_index": 4,
    "original_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.86
  },
  {
    "target_task": "qqp",
    "donor_task": "rte",
    "trial_index": "rev_4",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.46867167919798,
        "accuracy": 90.234375
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.82
  },
  {
    "target_task": "qqp",
    "donor_task": "stsb",
    "trial_index": "rev_0",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.26823238566132,
        "accuracy": 89.94140625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.84
  },
  {
    "target_task": "qqp",
    "donor_task": "stsb",
    "trial_index": "rev_4",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.19550281074329,
        "accuracy": 89.990234375
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "stsb",
    "donor_task": "qqp",
    "trial_index": 0,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.96830725441653,
        "spearman_corrcoef": 88.67616503909271
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.59143389109667,
        "spearman_corrcoef": 89.35968323428853
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.9
  },
  {
    "target_task": "qqp",
    "donor_task": "rte",
    "trial_index": "rev_1",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.1859296482412,
        "accuracy": 90.0390625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "stsb",
    "donor_task": "qqp",
    "trial_index": 4,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 90.34995245052271,
        "spearman_corrcoef": 90.2020417053629
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 90.70039217969462,
        "spearman_corrcoef": 90.5724118194741
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "qqp",
    "donor_task": "mrpc",
    "trial_index": "rev_1",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.25674827369743,
        "accuracy": 90.087890625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.88
  },
  {
    "target_task": "stsb",
    "donor_task": "qqp",
    "trial_index": 3,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "qqp",
    "donor_task": "stsb",
    "trial_index": "rev_3",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.44534665833854,
        "accuracy": 90.185546875
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.62
  },
  {
    "target_task": "mrpc",
    "donor_task": "qqp",
    "trial_index": 3,
    "original_score": {
      "mrpc": {
        "f1": 89.83957219251337,
        "accuracy": 86.02941176470588
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 90.65255731922399,
        "accuracy": 87.00980392156863
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "qqp",
    "donor_task": "mrpc",
    "trial_index": "rev_3",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.33459357277881,
        "accuracy": 90.185546875
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "mrpc",
    "donor_task": "qqp",
    "trial_index": 4,
    "original_score": {
      "mrpc": {
        "f1": 92.2242314647378,
        "accuracy": 89.4607843137255
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 92.28007181328547,
        "accuracy": 89.4607843137255
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "qqp",
    "donor_task": "mrpc",
    "trial_index": "rev_4",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.40648379052368,
        "accuracy": 90.13671875
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "stsb",
    "donor_task": "qqp",
    "trial_index": 1,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "sst2",
    "donor_task": "qqp",
    "trial_index": 0,
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.57798165137615
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "qqp",
    "donor_task": "stsb",
    "trial_index": "rev_1",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.06030150753769,
        "accuracy": 89.94140625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "stsb",
    "donor_task": "qqp",
    "trial_index": 2,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 89.61007975390662,
        "spearman_corrcoef": 89.45270247817778
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.6841254380529,
        "spearman_corrcoef": 89.61216533488587
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "qqp",
    "donor_task": "sst2",
    "trial_index": "rev_0",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.17624763108023,
        "accuracy": 90.087890625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "qqp",
    "donor_task": "stsb",
    "trial_index": "rev_2",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "qqp",
    "trial_index": 0,
    "original_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "cola",
    "donor_task": "qqp",
    "trial_index": 3,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "qqp",
    "donor_task": "cola",
    "trial_index": "rev_3",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.13745271122319,
        "accuracy": 90.0390625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "qqp",
    "donor_task": "mrpc",
    "trial_index": "rev_0",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.24072910119422,
        "accuracy": 90.087890625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "cola",
    "donor_task": "qqp",
    "trial_index": 0,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "mnli",
    "trial_index": 2,
    "original_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 19.133574007220215
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "rte",
    "donor_task": "qnli",
    "trial_index": 2,
    "original_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 77.9783393501805
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "qnli",
    "trial_index": 0,
    "original_score": {
      "rte": {
        "accuracy": 80.50541516245488
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 83.75451263537906
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 47.292418772563174
      }
    },
    "weighting": 0.54
  },
  {
    "target_task": "qnli",
    "donor_task": "rte",
    "trial_index": "rev_0",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 49.951171875
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 48.6328125
      }
    },
    "weighting": 0.38
  },
  {
    "target_task": "qnli",
    "donor_task": "rte",
    "trial_index": "rev_2",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 55.17578125
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 55.17578125
      }
    },
    "weighting": 0.0
  },
  {
    "target_task": "rte",
    "donor_task": "mnli",
    "trial_index": 0,
    "original_score": {
      "rte": {
        "accuracy": 80.50541516245488
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 86.64259927797833
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 44.765342960288805
      }
    },
    "weighting": 0.76
  },
  {
    "target_task": "mnli",
    "donor_task": "rte",
    "trial_index": "rev_0",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.939453125
      },
      "mnli_mismatched": {
        "accuracy": 88.0859375
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 26.7578125
      },
      "mnli_mismatched": {
        "accuracy": 27.685546875
      }
    },
    "weighting": 0.74
  },
  {
    "target_task": "cola",
    "donor_task": "qnli",
    "trial_index": 2,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "qnli",
    "donor_task": "cola",
    "trial_index": "rev_2",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 53.857421875
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 52.099609375
      }
    },
    "weighting": 0.58
  },
  {
    "target_task": "cola",
    "donor_task": "mnli",
    "trial_index": 2,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.93307552514323
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "cola",
    "donor_task": "qnli",
    "trial_index": 1,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.32614350622284
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.570012821477505
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "qnli",
    "donor_task": "cola",
    "trial_index": "rev_1",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 50.830078125
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.5859375
      }
    },
    "weighting": 0.46
  },
  {
    "target_task": "mnli",
    "donor_task": "cola",
    "trial_index": "rev_2",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.646484375
      },
      "mnli_mismatched": {
        "accuracy": 88.18359375
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 32.080078125
      },
      "mnli_mismatched": {
        "accuracy": 32.12890625
      }
    },
    "weighting": 0.82
  },
  {
    "target_task": "cola",
    "donor_task": "mnli",
    "trial_index": 1,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 62.32614350622284
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 62.352713855717354
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.28610000141659697
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "rte",
    "donor_task": "qnli",
    "trial_index": 3,
    "original_score": {
      "rte": {
        "accuracy": 83.39350180505414
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 84.83754512635379
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 47.292418772563174
      }
    },
    "weighting": 0.28
  },
  {
    "target_task": "rte",
    "donor_task": "mnli",
    "trial_index": 3,
    "original_score": {
      "rte": {
        "accuracy": 83.39350180505414
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 87.72563176895306
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.54151624548736
      }
    },
    "weighting": 0.72
  },
  {
    "target_task": "mnli",
    "donor_task": "cola",
    "trial_index": "rev_1",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.79296875
      },
      "mnli_mismatched": {
        "accuracy": 87.841796875
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 30.810546875
      },
      "mnli_mismatched": {
        "accuracy": 31.201171875
      }
    },
    "weighting": 0.8
  },
  {
    "target_task": "qnli",
    "donor_task": "rte",
    "trial_index": "rev_3",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 56.103515625
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 56.103515625
      }
    },
    "weighting": 0.0
  },
  {
    "target_task": "rte",
    "donor_task": "qnli",
    "trial_index": 4,
    "original_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.86
  },
  {
    "target_task": "rte",
    "donor_task": "mnli",
    "trial_index": 4,
    "original_score": {
      "rte": {
        "accuracy": 79.78339350180505
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 85.1985559566787
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 57.03971119133574
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "mnli",
    "donor_task": "rte",
    "trial_index": "rev_3",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.20703125
      },
      "mnli_mismatched": {
        "accuracy": 87.59765625
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 30.126953125
      },
      "mnli_mismatched": {
        "accuracy": 30.908203125
      }
    },
    "weighting": 0.92
  },
  {
    "target_task": "qnli",
    "donor_task": "rte",
    "trial_index": "rev_4",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 50.0
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.0
      }
    },
    "weighting": 0.0
  },
  {
    "target_task": "rte",
    "donor_task": "qnli",
    "trial_index": 1,
    "original_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "rte",
    "donor_task": "mnli",
    "trial_index": 1,
    "original_score": {
      "rte": {
        "accuracy": 82.67148014440433
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 83.03249097472924
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 19.133574007220215
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "mnli",
    "donor_task": "rte",
    "trial_index": "rev_4",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.646484375
      },
      "mnli_mismatched": {
        "accuracy": 88.28125
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 60.15625
      },
      "mnli_mismatched": {
        "accuracy": 60.302734375
      }
    },
    "weighting": 0.36
  },
  {
    "target_task": "qnli",
    "donor_task": "rte",
    "trial_index": "rev_1",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 52.587890625
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 51.904296875
      }
    },
    "weighting": 0.46
  },
  {
    "target_task": "mnli",
    "donor_task": "rte",
    "trial_index": "rev_1",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.841796875
      },
      "mnli_mismatched": {
        "accuracy": 87.841796875
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 30.615234375
      },
      "mnli_mismatched": {
        "accuracy": 31.0546875
      }
    },
    "weighting": 0.78
  },
  {
    "target_task": "qnli",
    "donor_task": "stsb",
    "trial_index": "rev_0",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 67.236328125
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 67.236328125
      }
    },
    "weighting": 0.0
  },
  {
    "target_task": "stsb",
    "donor_task": "qnli",
    "trial_index": 0,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.96830725441653,
        "spearman_corrcoef": 88.67616503909271
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.04982475021356,
        "spearman_corrcoef": 88.75664187964293
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "mnli",
    "donor_task": "stsb",
    "trial_index": "rev_0",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.353515625
      },
      "mnli_mismatched": {
        "accuracy": 87.6953125
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 26.171875
      },
      "mnli_mismatched": {
        "accuracy": 23.583984375
      }
    },
    "weighting": 0.86
  },
  {
    "target_task": "stsb",
    "donor_task": "qnli",
    "trial_index": 4,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 90.34995245052271,
        "spearman_corrcoef": 90.2020417053629
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 90.46140407454753,
        "spearman_corrcoef": 90.2742613255625
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.88
  },
  {
    "target_task": "qnli",
    "donor_task": "stsb",
    "trial_index": "rev_4",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 50.68359375
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 49.560546875
      }
    },
    "weighting": 0.72
  },
  {
    "target_task": "mrpc",
    "donor_task": "qqp",
    "trial_index": 1,
    "original_score": {
      "mrpc": {
        "f1": 92.7689594356261,
        "accuracy": 89.95098039215686
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.47442680776014,
        "accuracy": 90.93137254901961
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "mrpc",
    "donor_task": "qnli",
    "trial_index": 1,
    "original_score": {
      "mrpc": {
        "f1": 92.7689594356261,
        "accuracy": 89.95098039215686
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.12169312169313,
        "accuracy": 90.44117647058823
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.42
  },
  {
    "target_task": "mnli",
    "donor_task": "stsb",
    "trial_index": "rev_4",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.3046875
      },
      "mnli_mismatched": {
        "accuracy": 87.79296875
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 22.94921875
      },
      "mnli_mismatched": {
        "accuracy": 20.80078125
      }
    },
    "weighting": 0.86
  },
  {
    "target_task": "mrpc",
    "donor_task": "mnli",
    "trial_index": 1,
    "original_score": {
      "mrpc": {
        "f1": 92.7689594356261,
        "accuracy": 89.95098039215686
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.30985915492958,
        "accuracy": 90.68627450980392
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 2.816901408450704,
        "accuracy": 32.35294117647059
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "qnli",
    "donor_task": "mrpc",
    "trial_index": "rev_1",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 60.595703125
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 60.595703125
      }
    },
    "weighting": 0.0
  },
  {
    "target_task": "mnli",
    "donor_task": "mrpc",
    "trial_index": "rev_1",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.548828125
      },
      "mnli_mismatched": {
        "accuracy": 87.6953125
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 26.318359375
      },
      "mnli_mismatched": {
        "accuracy": 23.53515625
      }
    },
    "weighting": 0.9
  },
  {
    "target_task": "stsb",
    "donor_task": "mnli",
    "trial_index": 3,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": -54.898968458132735,
        "spearman_corrcoef": -53.62900031055566
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "stsb",
    "donor_task": "qnli",
    "trial_index": 3,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.45382839936683,
        "spearman_corrcoef": 88.50067687436707
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.54059464487439,
        "spearman_corrcoef": 88.60956862810639
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.8
  },
  {
    "target_task": "qnli",
    "donor_task": "stsb",
    "trial_index": "rev_3",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 51.708984375
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.08
  },
  {
    "target_task": "stsb",
    "donor_task": "mnli",
    "trial_index": 0,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 88.96830725441653,
        "spearman_corrcoef": 88.67616503909271
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 88.98379634126711,
        "spearman_corrcoef": 88.68865587930242
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "mrpc",
    "donor_task": "mnli",
    "trial_index": 3,
    "original_score": {
      "mrpc": {
        "f1": 89.83957219251337,
        "accuracy": 86.02941176470588
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 90.29982363315698,
        "accuracy": 86.51960784313727
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "mrpc",
    "donor_task": "qnli",
    "trial_index": 4,
    "original_score": {
      "mrpc": {
        "f1": 92.2242314647378,
        "accuracy": 89.4607843137255
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 92.2242314647378,
        "accuracy": 89.4607843137255
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "qnli",
    "donor_task": "mrpc",
    "trial_index": "rev_4",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 54.58984375
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.02
  },
  {
    "target_task": "mnli",
    "donor_task": "rte",
    "trial_index": "rev_2",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 88.037109375
      },
      "mnli_mismatched": {
        "accuracy": 87.841796875
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.8
  },
  {
    "target_task": "mnli",
    "donor_task": "mrpc",
    "trial_index": "rev_3",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.841796875
      },
      "mnli_mismatched": {
        "accuracy": 88.232421875
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.56
  },
  {
    "target_task": "stsb",
    "donor_task": "qnli",
    "trial_index": 1,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 91.1417238222836,
        "spearman_corrcoef": 90.90204685529068
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.72
  },
  {
    "target_task": "mrpc",
    "donor_task": "mnli",
    "trial_index": 4,
    "original_score": {
      "mrpc": {
        "f1": 92.2242314647378,
        "accuracy": 89.4607843137255
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 92.71758436944938,
        "accuracy": 89.95098039215686
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.94
  },
  {
    "target_task": "mnli",
    "donor_task": "mrpc",
    "trial_index": "rev_4",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.841796875
      },
      "mnli_mismatched": {
        "accuracy": 87.939453125
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.8
  },
  {
    "target_task": "qnli",
    "donor_task": "stsb",
    "trial_index": "rev_1",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 54.98046875
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.68
  },
  {
    "target_task": "stsb",
    "donor_task": "mnli",
    "trial_index": 1,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 91.14673434029001,
        "spearman_corrcoef": 90.89387343906313
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mrpc",
    "donor_task": "qnli",
    "trial_index": 3,
    "original_score": {
      "mrpc": {
        "f1": 89.83957219251337,
        "accuracy": 86.02941176470588
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 90.21739130434783,
        "accuracy": 86.76470588235294
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 0.52
  },
  {
    "target_task": "mnli",
    "donor_task": "stsb",
    "trial_index": "rev_1",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.109375
      },
      "mnli_mismatched": {
        "accuracy": 87.646484375
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.88
  },
  {
    "target_task": "stsb",
    "donor_task": "mnli",
    "trial_index": 2,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 89.61007975390662,
        "spearman_corrcoef": 89.45270247817778
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.56167570717469,
        "spearman_corrcoef": 89.5045548280306
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "stsb",
    "donor_task": "qnli",
    "trial_index": 2,
    "original_score": {
      "stsb": {
        "pearson_corrcoef": 89.61007975390662,
        "spearman_corrcoef": 89.45270247817778
      }
    },
    "merged_score": {
      "stsb": {
        "pearson_corrcoef": 89.58815002073264,
        "spearman_corrcoef": 89.48612925571153
      }
    },
    "donor_body_score": {
      "stsb": {
        "pearson_corrcoef": 0.0,
        "spearman_corrcoef": 0.0
      }
    },
    "weighting": 0.02
  },
  {
    "target_task": "qnli",
    "donor_task": "stsb",
    "trial_index": "rev_2",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 69.970703125
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.02
  },
  {
    "target_task": "mnli",
    "donor_task": "stsb",
    "trial_index": "rev_2",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.255859375
      },
      "mnli_mismatched": {
        "accuracy": 87.548828125
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.88
  },
  {
    "target_task": "sst2",
    "donor_task": "mnli",
    "trial_index": 0,
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "sst2",
    "donor_task": "qnli",
    "trial_index": 0,
    "original_score": {
      "sst2": {
        "accuracy": 93.34862385321101
      }
    },
    "merged_score": {
      "sst2": {
        "accuracy": 93.80733944954129
      }
    },
    "donor_body_score": {
      "sst2": {
        "accuracy": 49.08256880733945
      }
    },
    "weighting": 0.04
  },
  {
    "target_task": "cola",
    "donor_task": "mnli",
    "trial_index": 3,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 59.70593009825211
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.96
  },
  {
    "target_task": "mnli",
    "donor_task": "sst2",
    "trial_index": "rev_0",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.109375
      },
      "mnli_mismatched": {
        "accuracy": 87.548828125
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.78
  },
  {
    "target_task": "cola",
    "donor_task": "qnli",
    "trial_index": 3,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 59.55827154899262
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.22
  },
  {
    "target_task": "mrpc",
    "donor_task": "qnli",
    "trial_index": 0,
    "original_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "qnli",
    "donor_task": "cola",
    "trial_index": "rev_3",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 53.662109375
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.18
  },
  {
    "target_task": "mrpc",
    "donor_task": "mnli",
    "trial_index": 0,
    "original_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "merged_score": {
      "mrpc": {
        "f1": 93.09734513274337,
        "accuracy": 90.44117647058823
      }
    },
    "donor_body_score": {
      "mrpc": {
        "f1": 0.0,
        "accuracy": 31.61764705882353
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "mnli",
    "donor_task": "cola",
    "trial_index": "rev_3",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 88.28125
      },
      "mnli_mismatched": {
        "accuracy": 88.28125
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.54
  },
  {
    "target_task": "qnli",
    "donor_task": "sst2",
    "trial_index": "rev_0",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.0
  },
  {
    "target_task": "qnli",
    "donor_task": "mrpc",
    "trial_index": "rev_0",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 63.76953125
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.04
  },
  {
    "target_task": "cola",
    "donor_task": "mnli",
    "trial_index": 0,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 1.0
  },
  {
    "target_task": "cola",
    "donor_task": "qnli",
    "trial_index": 0,
    "original_score": {
      "cola": {
        "matthews_corrcoef": 61.781884857876335
      }
    },
    "merged_score": {
      "cola": {
        "matthews_corrcoef": 63.81799162684562
      }
    },
    "donor_body_score": {
      "cola": {
        "matthews_corrcoef": 0.0
      }
    },
    "weighting": 0.02
  },
  {
    "target_task": "qnli",
    "donor_task": "cola",
    "trial_index": "rev_0",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 53.857421875
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.5
  },
  {
    "target_task": "qqp",
    "donor_task": "cola",
    "trial_index": "rev_0",
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.1859296482412,
        "accuracy": 90.0390625
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.86
  },
  {
    "target_task": "mnli",
    "donor_task": "mrpc",
    "trial_index": "rev_0",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.98828125
      },
      "mnli_mismatched": {
        "accuracy": 88.232421875
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.52
  },
  {
    "target_task": "qqp",
    "donor_task": "qnli",
    "trial_index": 0,
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.27735368956742,
        "accuracy": 90.234375
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.18
  },
  {
    "target_task": "qqp",
    "donor_task": "mnli",
    "trial_index": 0,
    "original_score": {
      "qqp": {
        "f1": 87.01134930643127,
        "accuracy": 89.94140625
      }
    },
    "merged_score": {
      "qqp": {
        "f1": 87.35053492762744,
        "accuracy": 90.185546875
      }
    },
    "donor_body_score": {
      "qqp": {
        "f1": 0.0,
        "accuracy": 63.4765625
      }
    },
    "weighting": 0.98
  },
  {
    "target_task": "qnli",
    "donor_task": "qqp",
    "trial_index": "rev_0",
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 77.392578125
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.14
  },
  {
    "target_task": "mnli",
    "donor_task": "qqp",
    "trial_index": "rev_0",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 87.98828125
      },
      "mnli_mismatched": {
        "accuracy": 88.134765625
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.76
  },
  {
    "target_task": "mnli",
    "donor_task": "qnli",
    "trial_index": "rev_0",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 86.962890625
      },
      "mnli_mismatched": {
        "accuracy": 87.060546875
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.5
  },
  {
    "target_task": "qnli",
    "donor_task": "mnli",
    "trial_index": 0,
    "original_score": {
      "qnli": {
        "accuracy": 49.365234375
      }
    },
    "merged_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "donor_body_score": {
      "qnli": {
        "accuracy": 50.634765625
      }
    },
    "weighting": 0.0
  },
  {
    "target_task": "mnli",
    "donor_task": "cola",
    "trial_index": "rev_0",
    "original_score": {
      "mnli_matched": {
        "accuracy": 86.81640625
      },
      "mnli_mismatched": {
        "accuracy": 87.109375
      }
    },
    "merged_score": {
      "mnli_matched": {
        "accuracy": 88.232421875
      },
      "mnli_mismatched": {
        "accuracy": 87.98828125
      }
    },
    "donor_body_score": {
      "mnli_matched": {
        "accuracy": 35.595703125
      },
      "mnli_mismatched": {
        "accuracy": 34.86328125
      }
    },
    "weighting": 0.58
  }
]