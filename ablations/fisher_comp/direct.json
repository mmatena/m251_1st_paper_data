[
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.068592057761734
      }
    },
    "weighting": 0.78
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.74
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.76
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.3
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.26
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.151624548736464
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.62
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.068592057761734
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.46
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.34
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.151624548736464
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.6
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.64
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.95306859205776
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.87364620938629
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.58
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.62
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.87364620938629
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.64
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.34657039711191
      }
    },
    "weighting": 0.74
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.6
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.68
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.34657039711191
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.76
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.068592057761734
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.76
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.6
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.6
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.64
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.95306859205776
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.87364620938629
      }
    },
    "weighting": 0.68
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.46
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.3
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.151624548736464
      }
    },
    "weighting": 0.34
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.62093862815884
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.58
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.34657039711191
      }
    },
    "weighting": 0.68
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.68
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.86
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 49.09747292418773
      }
    },
    "weighting": 0.76
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.18050541516246
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 49.09747292418773
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.18050541516246
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.95306859205776
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.985559566786996
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.985559566786996
      }
    },
    "weighting": 0.62
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.624548736462096
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.624548736462096
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 49.09747292418773
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.985559566786996
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.18050541516246
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 48.375451263537904
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 48.375451263537904
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.624548736462096
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 48.375451263537904
      }
    },
    "weighting": 0.8
  }
]