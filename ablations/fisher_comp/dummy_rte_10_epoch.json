[
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 73.28519855595668
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.53429602888086
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.06498194945848
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.78
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.94
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.46
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.62
  }
]