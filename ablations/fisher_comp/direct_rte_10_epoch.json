[
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 48.375451263537904
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.62093862815884
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 48.375451263537904
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.79061371841155
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 71.84115523465704
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.98194945848375
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.79061371841155
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 72.56317689530685
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.068592057761734
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 72.20216606498195
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 55.95667870036101
      }
    },
    "weighting": 0.86
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 72.56317689530685
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.151624548736464
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 48.375451263537904
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.98
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.4259927797834
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.98
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 58.844765342960294
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.79061371841155
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.4259927797834
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.94
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 72.20216606498195
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.86
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.94
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.4259927797834
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 49.09747292418773
      }
    },
    "weighting": 0.98
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 71.84115523465704
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 71.84115523465704
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.151624548736464
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 72.20216606498195
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.068592057761734
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 72.20216606498195
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 55.95667870036101
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 73.28519855595668
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 72.92418772563177
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.068592057761734
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 72.56317689530685
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.151624548736464
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 72.92418772563177
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.53429602888086
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.53429602888086
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.89530685920577
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 73.28519855595668
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 55.95667870036101
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 71.84115523465704
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.84
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 49.09747292418773
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.53429602888086
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.53429602888086
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 55.23465703971119
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.53429602888086
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.4259927797834
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.82
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.06498194945848
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.78
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.17328519855594
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.82
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.51263537906137
      }
    },
    "weighting": 0.82
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.53429602888086
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.4259927797834
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 49.09747292418773
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.96
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.17328519855594
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.76
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.17328519855594
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 75.81227436823104
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 55.23465703971119
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.89530685920577
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.82
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.89530685920577
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.53429602888086
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 55.23465703971119
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 76.89530685920577
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.06498194945848
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.51263537906137
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.06498194945848
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.78
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.51263537906137
      }
    },
    "weighting": 0.82
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 66.78700361010831
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.54151624548736
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.54151624548736
      }
    },
    "weighting": 0.84
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.54151624548736
      }
    },
    "weighting": 0.82
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.3
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 60.28880866425993
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.068592057761734
      }
    },
    "weighting": 0.78
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.18050541516246
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.068592057761734
      }
    },
    "weighting": 0.82
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.151624548736464
      }
    },
    "weighting": 0.34
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.56
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.151624548736464
      }
    },
    "weighting": 0.54
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.46
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.18050541516246
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.74
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.82
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.84
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.50902527075813
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.46
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.62
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 49.09747292418773
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 49.09747292418773
      }
    },
    "weighting": 0.84
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.54
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.95306859205776
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.87364620938629
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.62
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.95306859205776
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.985559566786996
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.58
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.82
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.84
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.6
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.87364620938629
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.624548736462096
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.34657039711191
      }
    },
    "weighting": 0.78
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.34657039711191
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.84
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 49.09747292418773
      }
    },
    "weighting": 0.84
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.62
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.624548736462096
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.64
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.64
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.985559566786996
      }
    },
    "weighting": 0.68
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.5
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.23104693140795
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.6
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.56
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.151624548736464
      }
    },
    "weighting": 0.54
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 50.18050541516246
      }
    },
    "weighting": 0.32
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 1,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 57.761732851985556
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 67.14801444043322
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.36
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.70397111913357
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.068592057761734
      }
    },
    "weighting": 0.72
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.95306859205776
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 54.87364620938629
      }
    },
    "weighting": 0.68
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 0,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 62.8158844765343
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 65.34296028880865
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.76
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 2,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 68.59205776173285
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.985559566786996
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.88
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.62093862815884
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.62
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 48.375451263537904
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 51.624548736462096
      }
    },
    "weighting": 0.68
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.66
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.1768953068592
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.34657039711191
      }
    },
    "weighting": 0.7
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 48.375451263537904
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 392702
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 48.375451263537904
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 1024
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.9
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 256,
      "donor_fisher_examples": 32768
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.898916967509024
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 53.42960288808665
      }
    },
    "weighting": 0.92
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 1024,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.8
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 3,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 4096
    },
    "original_score": {
      "rte": {
        "accuracy": 59.92779783393502
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 63.537906137184116
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.68
  },
  {
    "task": "rte",
    "other_task": "mnli",
    "trial_index": 4,
    "hyperparams": {
      "target_fisher_examples": 2490,
      "donor_fisher_examples": 256
    },
    "original_score": {
      "rte": {
        "accuracy": 61.01083032490975
      }
    },
    "merged_score": {
      "rte": {
        "accuracy": 64.25992779783394
      }
    },
    "donor_body_score": {
      "rte": {
        "accuracy": 52.707581227436826
      }
    },
    "weighting": 0.92
  }
]